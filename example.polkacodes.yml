# This is an example configuration file for polka.codes
# Generated by polka.codes
#
# Config files are loaded in this order (later configs override earlier ones):
# 1. Global: ~/.config/polkacodes/config.yml
# 2. Project:  ./.polkacodes.yml (in current working directory)
# 3. CLI:     --config <path> (highest priority)

#============================================================================
# AI Provider Configurations
#============================================================================

providers:
  # Anthropic Claude configuration
  anthropic:
    apiKey: "sk-ant-..." # Your Anthropic API key
    defaultModel: "claude-sonnet-4-5-20250929" # Claude Sonnet 4.5
    defaultParameters:
      thinkingBudgetTokens: 8192 # Enable extended thinking mode

  # DeepSeek configuration (cost-effective)
  deepseek:
    apiKey: "sk-..." # Your DeepSeek API key
    defaultModel: "deepseek-chat"

  # OpenAI configuration
  openai:
    apiKey: "sk-..." # Your OpenAI API key
    defaultModel: "gpt-4o"

  # OpenAI-compatible provider configuration
  # Use this for any OpenAI-compatible API (e.g., Together AI, Anyscale, Groq, etc.)
  openai-compatible:
    apiKey: "your-api-key"
    defaultModel: "gpt-4o"
    name: "Together AI"  # Optional: Custom name for the provider
    baseUrl: "https://api.together.xyz/v1"  # Replace with your provider's base URL
    # Other examples:
    # name: "Anyscale"
    # baseUrl: "https://api.anyscale.com/v1"
    # name: "Groq"
    # baseUrl: "https://api.groq.com/openai/v1"
    # name: "Custom Provider"
    # baseUrl: "https://your-custom-endpoint.com/v1"

  # OpenRouter configuration (access to many models)
  openrouter:
    apiKey: "sk-or-..." # Your OpenRouter API key
    defaultModel: "anthropic/claude-sonnet-4"

  # Local Ollama configuration
  ollama:
    defaultModel: "qwen2.5-coder:7b"

  # Google Vertex AI configuration
  google-vertex:
    project: "your-gcp-project-id"
    location: "us-central1"
    keyFile: "/path/to/service-account-key.json" # Optional
    defaultModel: "gemini-2.5-pro"

# Default provider to use when not specified elsewhere
defaultProvider: "anthropic"

# Default model to use
defaultModel: "claude-sonnet-4-5-20250929"

#============================================================================
# Request Settings
#============================================================================

# Number of retries for failed AI requests
retryCount: 3

# Timeout in seconds for AI requests
requestTimeoutSeconds: 120

# Default parameters for AI requests
defaultParameters:
  temperature: 0.7
  maxTokens: 8192

# Maximum number of messages for AI operations
maxMessageCount: 50

# Threshold for summarizing conversation history
summaryThreshold: 20

#============================================================================
# Budget Management
#============================================================================

# Overall budget limit in USD
budget: 10.0

#============================================================================
# Custom Scripts
#============================================================================
# Scripts that can be executed by AI agents
# Simple format: just the command
# Complex format: command with description and metadata

scripts:
  # Testing
  test: "bun test"
  test:watch: "bun test --watch"

  # Code quality
  format: "bun run format"
  check: "bun run check"
  typecheck: "bun run typecheck"
  lint: "bun run lint"

  # Build
  build:
    command: "bun run build"
    description: "Build the project for production"

  # Complex script with workflow
  verify:
    workflow: "test"
    description: "Run all verification checks"
    input:
      files: ["src/**/*.ts"]

#============================================================================
# Command-Specific Overrides
#============================================================================
# Override provider/model for specific commands

commands:
  default:
    provider: "anthropic"
    model: "claude-sonnet-4-5-20250929"

  # Use faster/cheaper model for commits
  commit:
    provider: "anthropic"
    model: "claude-haiku-4.5-20250929"
    budget: 0.5

  # Use powerful model for PR reviews
  pr:
    provider: "anthropic"
    model: "claude-sonnet-4-5-20250929"
    budget: 5.0

  # Local model for code generation
  code:
    provider: "ollama"
    model: "qwen2.5-coder:7b"

  # Use OpenAI-compatible provider for specific commands
  review:
    provider: "openai-compatible"
    model: "gpt-4o"
    name: "Together AI"
    baseUrl: "https://api.together.xyz/v1"

#============================================================================
# Tool-Specific Overrides
#============================================================================
# Override provider/model for specific tools

tools:
  # Use Gemini for web search
  search:
    provider: "google-vertex"
    model: "gemini-2.5-pro"

  # Use local model for file operations
  readFile:
    provider: "ollama"
    model: "qwen2.5-coder:7b"

#============================================================================
# Custom Rules
#============================================================================
# Rules that guide AI behavior
# Can be strings, file paths, URLs, or GitHub repos

rules:
  - "Use TypeScript for all new code"
  - "Follow the project's existing code style"
  - "Write tests for all new functionality"
  - "Add JSDoc comments for public APIs"
  - "Keep functions small and focused"

  # Load from local file
  - path: "docs/contributing.md"

  # Load from URL
  - url: "https://example.com/coding-standards.md"

  # Load from GitHub repo
  - repo: "polka-codes/polka"
    path: "rules/security.md"
    branch: "main"

#============================================================================
# MCP Servers
#============================================================================
# Model Context Protocol servers for additional tools and capabilities

mcpServers:
  # Filesystem access server
  filesystem:
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-filesystem", "/allowed/path"]

  # GitHub integration
  github:
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-github"]
    env:
      GITHUB_TOKEN: "ghp_..."

  # Database access
  postgres:
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-postgres", "postgresql://localhost/db"]
    tools:
      query:
        provider: "anthropic"
        model: "claude-haiku-4.5-20250929"

#============================================================================
# Autonomous Agent Configuration
#============================================================================
# Configuration for the autonomous agent command

agent:
  # Configuration preset to use (conservative | balanced | aggressive |
  # continuous-improvement | working-dir)
  preset: "balanced"

  # Strategy: goal-directed (one-time goal) or continuous-improvement
  strategy: "goal-directed"

  # Continue running after goal completion
  continueOnCompletion: false

  # Maximum number of tasks to execute (0 = unlimited)
  maxIterations: 10

  # Timeout per task in milliseconds (0 = no timeout)
  timeout: 300000

  # Approval requirements: none | destructive | commits | all
  requireApprovalFor: "destructive"

  # Auto-approve safe (non-destructive) tasks
  autoApproveSafeTasks: true

  # Maximum cost for auto-approved tasks
  maxAutoApprovalCost: 5.0

  # Pause on error
  pauseOnError: true

  # Git branch for agent work
  workingBranch: "polka-agent"

  # Destructive operations requiring approval
  destructiveOperations:
    - "delete"
    - "force-push"
    - "reset"

  # Maximum concurrent tasks
  maxConcurrency: 1

  # Auto-save state interval (ms)
  autoSaveInterval: 30000

  # Working directory for task file creation
  workingDir: "./tasks"

  # Continuous improvement settings
  continuousImprovement:
    sleepTimeOnNoTasks: 60000      # Wait 1 minute when no tasks found
    sleepTimeBetweenTasks: 5000    # Wait 5 seconds between tasks
    maxCycles: 0                   # 0 = infinite cycles

  # Task discovery settings
  discovery:
    enabledStrategies:
      - "build-errors"
      - "failing-tests"
      - "type-errors"
      - "lint-issues"
      - "working-dir"              # Monitor working directory for plans
    cacheTime: 300000              # Cache discovery results for 5 minutes
    checkChanges: true             # Check for git changes

  # Safety checks
  safety:
    enabledChecks: []
    blockDestructive: true
    maxFileSize: 10485760          # 10MB

  # Health monitoring
  healthCheck:
    enabled: false
    interval: 60000                # Check every minute

#============================================================================
# Pricing Information
#============================================================================
# Cost per million tokens in USD
# Used for budget tracking and cost estimation

prices:
  # Anthropic Claude models
  anthropic:
    # Claude Sonnet 4.5 (latest)
    "claude-sonnet-4-5-20250929":
      inputPrice: 3.0
      outputPrice: 15.0
      cacheWritesPrice: 3.75
      cacheReadsPrice: 0.3

    # Claude Haiku 4.5 (fastest)
    "claude-haiku-4.5-20250929":
      inputPrice: 0.8
      outputPrice: 4.0
      cacheWritesPrice: 1.0
      cacheReadsPrice: 0.08

    # Claude Opus 4.5 (most capable)
    "claude-opus-4-5-20251101":
      inputPrice: 15.0
      outputPrice: 75.0
      cacheWritesPrice: 18.75
      cacheReadsPrice: 1.5

  # DeepSeek models (cost-effective)
  deepseek:
    "deepseek-chat":
      inputPrice: 0.0
      outputPrice: 1.1
      cacheWritesPrice: 0.27
      cacheReadsPrice: 0.07

    "deepseek-reasoner":
      inputPrice: 0.0
      outputPrice: 2.19
      cacheWritesPrice: 0.55
      cacheReadsPrice: 0.14

  # OpenAI models
  openai:
    "gpt-4o":
      inputPrice: 2.5
      outputPrice: 10.0
      cacheWritesPrice: 1.25
      cacheReadsPrice: 0.31

    "gpt-4o-mini":
      inputPrice: 0.15
      outputPrice: 0.6
      cacheWritesPrice: 0.075
      cacheReadsPrice: 0.019

  # OpenAI-compatible providers
  # Pricing varies by provider - check your provider's pricing
  # Common OpenAI-compatible providers:
  openai-compatible:
    # Together AI (example pricing - check actual rates)
    "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo":
      inputPrice: 0.59
      outputPrice: 0.79
      cacheWritesPrice: 0
      cacheReadsPrice: 0

    # Groq (example pricing - check actual rates)
    "llama-3.3-70b-versatile":
      inputPrice: 0.59
      outputPrice: 0.79
      cacheWritesPrice: 0
      cacheReadsPrice: 0

    # Anyscale (example pricing - check actual rates)
    "meta-llama/Llama-3-70b-chat-hf":
      inputPrice: 0.5
      outputPrice: 0.5
      cacheWritesPrice: 0
      cacheReadsPrice: 0

  # Google Gemini models
  google-vertex:
    "gemini-2.5-pro":
      inputPrice: 3.0
      outputPrice: 9.0
      cacheWritesPrice: 2.25
      cacheReadsPrice: 0.3

    "gemini-2.5-flash":
      inputPrice: 0.075
      outputPrice: 0.3
      cacheWritesPrice: 0.056
      cacheReadsPrice: 0.007

#============================================================================
# File Exclusions
#============================================================================
# Patterns to exclude from AI operations
# Supports glob patterns

excludeFiles:
  - "node_modules/**"
  - "dist/**"
  - "build/**"
  - ".git/**"
  - "*.log"
  - ".env"
  - ".env.*"
  - "coverage/**"
  - ".next/**"
  - ".turbo/**"
  - "*.min.js"
  - "*.min.css"
  - "package-lock.json"
  - "yarn.lock"
  - "pnpm-lock.yaml"
