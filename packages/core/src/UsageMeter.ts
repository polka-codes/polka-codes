/**
 * UsageMeter class for tracking API usage metrics
 * Generated by polka.codes
 */

import type { LanguageModelV2, LanguageModelV2Usage } from '@ai-sdk/provider'

export type ModelInfo = {
  inputPrice: number
  outputPrice: number
  cacheWritesPrice: number
  cacheReadsPrice: number
}

type Totals = { input: number; output: number; cachedRead: number; cost: number; messageCount: number }

type ProviderMetadataEntry = {
  provider: string
  model: string
  metadata: any
  timestamp: number
}

/**
 * Tracks token / cost usage across any mix of LLM models.
 * Supports optional caps on total messages and total cost.
 */
export class UsageMeter {
  #totals: Totals = { input: 0, output: 0, cachedRead: 0, cost: 0, messageCount: 0 }
  #providerMetadataEntries: ProviderMetadataEntry[] = []

  readonly #modelInfos: Record<string, ModelInfo>
  readonly #maxMessages: number
  readonly #maxCost: number

  constructor(modelInfos: Record<string, Record<string, Partial<ModelInfo>>> = {}, opts: { maxMessages?: number; maxCost?: number } = {}) {
    const infos: Record<string, ModelInfo> = {}
    for (const [provider, providerInfo] of Object.entries(modelInfos)) {
      for (const [model, modelInfo] of Object.entries(providerInfo)) {
        // make google-vertex to google
        // and unify modelId
        infos[`${provider.split('-')[0]}:${model.replace(/[.-]/g, '')}`] = {
          inputPrice: modelInfo.inputPrice ?? 0,
          outputPrice: modelInfo.outputPrice ?? 0,
          cacheWritesPrice: modelInfo.cacheWritesPrice ?? 0,
          cacheReadsPrice: modelInfo.cacheReadsPrice ?? 0,
        }
      }
    }

    this.#modelInfos = infos
    this.#maxMessages = opts.maxMessages ?? 1000
    this.#maxCost = opts.maxCost ?? 100
  }

  #calculateUsage(usage: LanguageModelV2Usage, providerMetadata: any, modelInfo: ModelInfo) {
    const providerMetadataKey = Object.keys(providerMetadata ?? {})[0]
    const metadata = providerMetadata?.[providerMetadataKey] ?? {}

    switch (providerMetadataKey) {
      case 'openrouter':
        return {
          input: usage.inputTokens ?? 0,
          output: usage.outputTokens ?? 0,
          cachedRead: usage.cachedInputTokens ?? 0,
          cost: metadata.usage?.cost ?? 0,
        }
      case 'anthropic': {
        const cachedRead = usage.cachedInputTokens ?? 0
        const cacheWrite = metadata?.promptCacheMissTokens ?? 0
        const input = usage.inputTokens ?? 0
        const output = usage.outputTokens ?? 0

        return {
          input: input + cacheWrite + cachedRead,
          output,
          cachedRead,
          cost:
            (input * modelInfo.inputPrice +
              output * modelInfo.outputPrice +
              cacheWrite * modelInfo.cacheWritesPrice +
              cachedRead * modelInfo.cacheReadsPrice) /
            1_000_000,
        }
      }
      case 'deepseek': {
        const cachedRead = usage.cachedInputTokens ?? 0
        const cacheWrite = metadata.promptCacheMissTokens ?? 0
        const input = usage.inputTokens ?? 0
        const output = usage.outputTokens ?? 0

        return {
          input,
          output,
          cachedRead,
          cost: (output * modelInfo.outputPrice + cacheWrite * modelInfo.inputPrice + cachedRead * modelInfo.cacheReadsPrice) / 1_000_000,
        }
      }
      default: {
        const cachedRead = usage.cachedInputTokens ?? 0
        const input = usage.inputTokens ?? 0
        const output = usage.outputTokens ?? 0

        return {
          input,
          output,
          cachedRead,
          cost: (input * modelInfo.inputPrice + output * modelInfo.outputPrice) / 1_000_000,
        }
      }
    }
  }

  addUsage(
    llm: LanguageModelV2,
    resp: { usage: LanguageModelV2Usage; providerMetadata?: any } | { totalUsage: LanguageModelV2Usage; providerMetadata?: any },
    options: { modelInfo?: ModelInfo } = {},
  ) {
    const modelInfo = options.modelInfo ??
      // make google.vertex.chat to google
      // and anthropic.messages to anthropic
      this.#modelInfos[`${llm.provider.split('.')[0]}:${llm.modelId.replace(/[.-]/g, '')}`] ?? {
        inputPrice: 0,
        outputPrice: 0,
        cacheWritesPrice: 0,
        cacheReadsPrice: 0,
      }

    const usage = 'totalUsage' in resp ? resp.totalUsage : resp.usage
    const result = this.#calculateUsage(usage, resp.providerMetadata, modelInfo)

    this.#totals.input += result.input || 0
    this.#totals.output += result.output || 0
    this.#totals.cachedRead += result.cachedRead || 0
    this.#totals.cost += result.cost || 0
    this.#totals.messageCount += 1

    // Store provider metadata for analytics
    if (resp.providerMetadata && Object.keys(resp.providerMetadata).length > 0) {
      const providerKey = Object.keys(resp.providerMetadata)[0]
      this.#providerMetadataEntries.push({
        provider: providerKey || llm.provider,
        model: llm.modelId,
        metadata: resp.providerMetadata[providerKey] || resp.providerMetadata,
        timestamp: Date.now(),
      })
    }
  }

  /** Override the running totals (e.g., restore from saved state). */
  setUsage(newUsage: Partial<Totals>, options: { clearMetadata?: boolean } = {}) {
    if (newUsage.input != null) this.#totals.input = newUsage.input
    if (newUsage.output != null) this.#totals.output = newUsage.output
    if (newUsage.cachedRead != null) this.#totals.cachedRead = newUsage.cachedRead
    if (newUsage.cost != null) this.#totals.cost = newUsage.cost
    if (newUsage.messageCount != null) this.#totals.messageCount = newUsage.messageCount
    if (options.clearMetadata) {
      this.#providerMetadataEntries = []
    }
  }

  /** Manually bump the message count (useful if you record some messages without token info). */
  incrementMessageCount(n = 1) {
    this.#totals.messageCount += n
  }

  /** Reset the running totals. */
  resetUsage() {
    this.#totals = { input: 0, output: 0, cachedRead: 0, cost: 0, messageCount: 0 }
    this.#providerMetadataEntries = []
  }

  /** Return true once either messages or cost exceed the configured caps. */
  isLimitExceeded() {
    const messageCount = this.#maxMessages > 0 && this.#totals.messageCount >= this.#maxMessages
    const cost = this.#maxCost > 0 && this.#totals.cost >= this.#maxCost

    return {
      messageCount,
      maxMessages: this.#maxMessages,
      cost,
      maxCost: this.#maxCost,
      result: messageCount || cost,
    }
  }

  /** Same as isLimitExceeded but throws an error if a limit is hit. */
  checkLimit() {
    const result = this.isLimitExceeded()
    if (result.result) {
      throw new Error(
        `Usage limit exceeded. Message count: ${result.messageCount}/${result.maxMessages}, cost: ${result.cost}/${result.maxCost}`,
      )
    }
  }

  /** Getter for the aggregated totals (immutable copy). */
  get usage() {
    return { ...this.#totals }
  }

  /** Getter for provider metadata entries (immutable copy). */
  get providerMetadata() {
    return [...this.#providerMetadataEntries]
  }

  /** Calculate cache statistics from stored metadata entries. */
  get cacheStats() {
    const entries = this.#providerMetadataEntries
    const totalRequests = entries.length

    let totalCachedTokens = 0
    let requestsWithCache = 0

    for (const entry of entries) {
      const metadata = entry.metadata
      // Check for cache hit fields only (NOT cache misses)
      // OpenAI: cachedPromptTokens
      // Anthropic: cacheReadTokens (prompt_cache_hit_tokens in newer versions)
      // DeepSeek: prompt_cache_hit_tokens (if available)

      // Guard against metadata being a primitive value (number/string)
      // This can happen if a provider returns flat metadata
      if (typeof metadata !== 'object' || metadata === null) {
        continue
      }

      const cachedTokens = metadata.cachedPromptTokens ?? metadata.cacheReadTokens ?? metadata.prompt_cache_hit_tokens ?? 0

      if (cachedTokens > 0) {
        totalCachedTokens += cachedTokens
        requestsWithCache++
      }
    }

    const cacheHitRate = totalRequests > 0 ? requestsWithCache / totalRequests : 0

    return {
      totalCachedTokens,
      totalRequests,
      requestsWithCache,
      cacheHitRate,
      entries: [...this.#providerMetadataEntries],
    }
  }

  /** Clear stored provider metadata entries. */
  clearProviderMetadata() {
    this.#providerMetadataEntries = []
  }

  /** Merge another UsageMeter's totals into this one. */
  merge(other: UsageMeter) {
    const otherUsage = other.usage
    this.#totals.input += otherUsage.input
    this.#totals.output += otherUsage.output
    this.#totals.cachedRead += otherUsage.cachedRead
    this.#totals.cost += otherUsage.cost
    this.#totals.messageCount += otherUsage.messageCount
    this.#providerMetadataEntries.push(...other.providerMetadata)
  }

  getUsageText() {
    const u = this.usage
    return `Usage - messages: ${u.messageCount}, input: ${u.input}, cached: ${u.cachedRead}, output: ${u.output}, cost: $${u.cost.toFixed(4)}`
  }

  onFinishHandler(llm: LanguageModelV2) {
    return (evt: { totalUsage: LanguageModelV2Usage; providerMetadata: any }) => {
      this.addUsage(llm, evt)
    }
  }
}
