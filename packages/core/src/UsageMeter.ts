/**
 * UsageMeter class for tracking API usage metrics
 * Generated by polka.codes
 */

import type { LanguageModelV2, LanguageModelV2Usage } from '@ai-sdk/provider'

export type ModelInfo = {
  inputPrice: number
  outputPrice: number
  cacheWritesPrice: number
  cacheReadsPrice: number
}

type Totals = { input: number; output: number; cachedRead: number; cost: number; messageCount: number }

/**
 * Tracks token / cost usage across any mix of LLM models.
 * Supports optional caps on total messages and total cost.
 */
export class UsageMeter {
  #totals: Totals = { input: 0, output: 0, cachedRead: 0, cost: 0, messageCount: 0 }

  readonly #modelInfos: Record<string, ModelInfo>
  readonly #maxMessages: number
  readonly #maxCost: number

  constructor(modelInfos: Record<string, Record<string, Partial<ModelInfo>>> = {}, opts: { maxMessages?: number; maxCost?: number } = {}) {
    const infos: Record<string, ModelInfo> = {}
    for (const [provider, providerInfo] of Object.entries(modelInfos)) {
      for (const [model, modelInfo] of Object.entries(providerInfo)) {
        // make google-vertex to google
        // and unify modelId
        infos[`${provider.split('-')[0]}:${model.replace(/[.-]/g, '')}`] = {
          inputPrice: modelInfo.inputPrice ?? 0,
          outputPrice: modelInfo.outputPrice ?? 0,
          cacheWritesPrice: modelInfo.cacheWritesPrice ?? 0,
          cacheReadsPrice: modelInfo.cacheReadsPrice ?? 0,
        }
      }
    }

    this.#modelInfos = infos
    this.#maxMessages = opts.maxMessages ?? 1000
    this.#maxCost = opts.maxCost ?? 100
  }

  #calculageUsage(usage: LanguageModelV2Usage, providerMetadata: any, modelInfo: ModelInfo) {
    const providerMetadataKey = Object.keys(providerMetadata ?? {})[0]
    const metadata = providerMetadata?.[providerMetadataKey] ?? {}

    switch (providerMetadataKey) {
      case 'openrouter':
        return {
          input: usage.inputTokens ?? 0,
          output: usage.outputTokens ?? 0,
          cachedRead: usage.cachedInputTokens ?? 0,
          cost: metadata.usage?.cost ?? 0,
        }
      case 'anthropic': {
        const cachedRead = usage.cachedInputTokens ?? 0
        const cacheWrite = metadata?.promptCacheMissTokens ?? 0
        const input = usage.inputTokens ?? 0
        const output = usage.outputTokens ?? 0

        return {
          input: input + cacheWrite + cachedRead,
          output,
          cachedRead,
          cost:
            (input * modelInfo.inputPrice +
              output * modelInfo.outputPrice +
              cacheWrite * modelInfo.cacheWritesPrice +
              cachedRead * modelInfo.cacheReadsPrice) /
            1_000_000,
        }
      }
      case 'deepseek': {
        const cachedRead = usage.cachedInputTokens ?? 0
        const cacheWrite = metadata.promptCacheMissTokens ?? 0
        const input = usage.inputTokens ?? 0
        const output = usage.outputTokens ?? 0

        return {
          input,
          output,
          cachedRead,
          cost: (output * modelInfo.outputPrice + cacheWrite * modelInfo.inputPrice + cachedRead * modelInfo.cacheReadsPrice) / 1_000_000,
        }
      }
      default: {
        const cachedRead = usage.cachedInputTokens ?? 0
        const input = usage.inputTokens ?? 0
        const output = usage.outputTokens ?? 0

        return {
          input,
          output,
          cachedRead,
          cost: (input * modelInfo.inputPrice + output * modelInfo.outputPrice) / 1_000_000,
        }
      }
    }
  }

  addUsage(
    llm: LanguageModelV2,
    resp: { usage: LanguageModelV2Usage; providerMetadata?: any } | { totalUsage: LanguageModelV2Usage; providerMetadata?: any },
    options: { modelInfo?: ModelInfo } = {},
  ) {
    const modelInfo = options.modelInfo ??
      // make google.vertex.chat to google
      // and anthropic.messages to anthropic
      this.#modelInfos[`${llm.provider.split('.')[0]}:${llm.modelId.replace(/[.-]/g, '')}`] ?? {
        inputPrice: 0,
        outputPrice: 0,
        cacheWritesPrice: 0,
        cacheReadsPrice: 0,
      }

    const usage = 'totalUsage' in resp ? resp.totalUsage : resp.usage
    const result = this.#calculageUsage(usage, resp.providerMetadata, modelInfo)

    this.#totals.input += result.input || 0
    this.#totals.output += result.output || 0
    this.#totals.cachedRead += result.cachedRead || 0
    this.#totals.cost += result.cost || 0
    this.#totals.messageCount += 1
  }

  /** Override the running totals (e.g., restore from saved state). */
  setUsage(newUsage: Partial<Totals>) {
    if (newUsage.input != null) this.#totals.input = newUsage.input
    if (newUsage.output != null) this.#totals.output = newUsage.output
    if (newUsage.cachedRead != null) this.#totals.cachedRead = newUsage.cachedRead
    if (newUsage.cost != null) this.#totals.cost = newUsage.cost
    if (newUsage.messageCount != null) this.#totals.messageCount = newUsage.messageCount
  }

  /** Manually bump the message count (useful if you record some messages without token info). */
  incrementMessageCount(n = 1) {
    this.#totals.messageCount += n
  }

  /** Reset the running totals. */
  resetUsage() {
    this.#totals = { input: 0, output: 0, cachedRead: 0, cost: 0, messageCount: 0 }
  }

  /** Return true once either messages or cost exceed the configured caps. */
  isLimitExceeded() {
    const messageCount = this.#maxMessages > 0 && this.#totals.messageCount >= this.#maxMessages
    const cost = this.#maxCost > 0 && this.#totals.cost >= this.#maxCost

    return {
      messageCount,
      maxMessages: this.#maxMessages,
      cost,
      maxCost: this.#maxCost,
      result: messageCount || cost,
    }
  }

  /** Same as isLimitExceeded but throws an error if a limit is hit. */
  checkLimit() {
    const result = this.isLimitExceeded()
    if (result.result) {
      throw new Error(
        `Usage limit exceeded. Message count: ${result.messageCount}/${result.maxMessages}, cost: ${result.cost}/${result.maxCost}`,
      )
    }
  }

  /** Getter for the aggregated totals (immutable copy). */
  get usage() {
    return { ...this.#totals }
  }

  /** Print a concise usage summary to console. */
  printUsage(customConsole: Console = console) {
    const u = this.usage
    customConsole.log(
      `Usage - messages: ${u.messageCount}, input: ${u.input}, cached: ${u.cachedRead}, ` +
        `output: ${u.output}, cost: $${u.cost.toFixed(4)}`,
    )
  }

  onFinishHandler(llm: LanguageModelV2) {
    return (evt: { totalUsage: LanguageModelV2Usage; providerMetadata: any }) => {
      this.addUsage(llm, evt)
    }
  }
}
